{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9548960,"sourceType":"datasetVersion","datasetId":5817971}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-05T00:33:51.791781Z","iopub.execute_input":"2024-10-05T00:33:51.792165Z","iopub.status.idle":"2024-10-05T00:33:52.158173Z","shell.execute_reply.started":"2024-10-05T00:33:51.792128Z","shell.execute_reply":"2024-10-05T00:33:52.157291Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bengali-news-article-summarization/preprocessed_bangla_news.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom transformers import MT5ForConditionalGeneration, AutoTokenizer ,DataCollatorForSeq2Seq, Trainer, TrainingArguments\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:33:52.159901Z","iopub.execute_input":"2024-10-05T00:33:52.160303Z","iopub.status.idle":"2024-10-05T00:34:13.204976Z","shell.execute_reply.started":"2024-10-05T00:33:52.160262Z","shell.execute_reply":"2024-10-05T00:34:13.204147Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset overview\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/bengali-news-article-summarization/preprocessed_bangla_news.csv', usecols=['summary', 'article'])","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:13.206105Z","iopub.execute_input":"2024-10-05T00:34:13.206713Z","iopub.status.idle":"2024-10-05T00:34:24.272453Z","shell.execute_reply.started":"2024-10-05T00:34:13.206677Z","shell.execute_reply":"2024-10-05T00:34:24.271614Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:24.273749Z","iopub.execute_input":"2024-10-05T00:34:24.274385Z","iopub.status.idle":"2024-10-05T00:34:24.312700Z","shell.execute_reply.started":"2024-10-05T00:34:24.274337Z","shell.execute_reply":"2024-10-05T00:34:24.311746Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 39141 entries, 0 to 39140\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   article  39141 non-null  object\n 1   summary  39141 non-null  object\ndtypes: object(2)\nmemory usage: 611.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:24.315964Z","iopub.execute_input":"2024-10-05T00:34:24.316543Z","iopub.status.idle":"2024-10-05T00:34:24.346117Z","shell.execute_reply.started":"2024-10-05T00:34:24.316508Z","shell.execute_reply":"2024-10-05T00:34:24.345232Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                 article  \\\n0      তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...   \n1      সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...   \n2      শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...   \n3      পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...   \n4      বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...   \n...                                                  ...   \n39136  আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...   \n39137  সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...   \n39138  দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...   \n39139  ‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...   \n39140  দেশে পুলিশের যেমন সুনাম আছে, তেমনি দুর্নামও আছ...   \n\n                                                 summary  \n0      তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...  \n1      সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...  \n2      শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...  \n3      পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...  \n4      বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...  \n...                                                  ...  \n39136  আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...  \n39137  সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...  \n39138  দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...  \n39139  ‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...  \n39140  দেশে পুলিশের যেমন সুনাম আছে, তেমনি দুর্নামও আছ...  \n\n[39141 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...</td>\n      <td>তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...</td>\n      <td>সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...</td>\n      <td>শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...</td>\n      <td>পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...</td>\n      <td>বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39136</th>\n      <td>আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...</td>\n      <td>আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...</td>\n    </tr>\n    <tr>\n      <th>39137</th>\n      <td>সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...</td>\n      <td>সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...</td>\n    </tr>\n    <tr>\n      <th>39138</th>\n      <td>দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...</td>\n      <td>দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...</td>\n    </tr>\n    <tr>\n      <th>39139</th>\n      <td>‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...</td>\n      <td>‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...</td>\n    </tr>\n    <tr>\n      <th>39140</th>\n      <td>দেশে পুলিশের যেমন সুনাম আছে, তেমনি দুর্নামও আছ...</td>\n      <td>দেশে পুলিশের যেমন সুনাম আছে, তেমনি দুর্নামও আছ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>39141 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocessing data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.30, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:24.347281Z","iopub.execute_input":"2024-10-05T00:34:24.347591Z","iopub.status.idle":"2024-10-05T00:34:24.368394Z","shell.execute_reply.started":"2024-10-05T00:34:24.347560Z","shell.execute_reply":"2024-10-05T00:34:24.367696Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nds_train = Dataset.from_pandas(df_train)\nds_test = Dataset.from_pandas(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:24.369481Z","iopub.execute_input":"2024-10-05T00:34:24.369778Z","iopub.status.idle":"2024-10-05T00:34:26.459911Z","shell.execute_reply.started":"2024-10-05T00:34:24.369746Z","shell.execute_reply":"2024-10-05T00:34:26.458865Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n\ntokenizer = MBart50TokenizerFast.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:26.461116Z","iopub.execute_input":"2024-10-05T00:34:26.461426Z","iopub.status.idle":"2024-10-05T00:34:45.497219Z","shell.execute_reply.started":"2024-10-05T00:34:26.461393Z","shell.execute_reply":"2024-10-05T00:34:45.496308Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9c245fb995469f95ae8f4b715f83b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5de0b7c8888a4040abd5a51cb783657d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1817b53330b1457ab1fe91f7730357ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f0e5c734e97480a9a618edc92954692"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5c7f2e16e14004ac38b9c821514df0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9d14a453ba447998e637f34551a78d"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_data(data):\n    input_features = tokenizer(data['article'], truncation=True, max_length=1024)\n    label = tokenizer(data['summary'],truncation =True, max_length=128)\n    return {\n        'input_ids':input_features['input_ids'],\n        'attention_mask': input_features['attention_mask'],\n        'labels': label['input_ids'],\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:45.498805Z","iopub.execute_input":"2024-10-05T00:34:45.499612Z","iopub.status.idle":"2024-10-05T00:34:45.506058Z","shell.execute_reply.started":"2024-10-05T00:34:45.499565Z","shell.execute_reply":"2024-10-05T00:34:45.505017Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_ds_train = ds_train.map(\n    tokenize_data,\n    batched=True,\n    remove_columns=ds_train.column_names  # Remove original columns\n)\n\n# Apply tokenization to the test dataset\ntokenized_ds_test = ds_test.map(\n    tokenize_data,\n    batched=True,\n    remove_columns=ds_test.column_names\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:34:45.507503Z","iopub.execute_input":"2024-10-05T00:34:45.508083Z","iopub.status.idle":"2024-10-05T00:35:28.658922Z","shell.execute_reply.started":"2024-10-05T00:34:45.508037Z","shell.execute_reply":"2024-10-05T00:35:28.657992Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27398 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541316173a8345b892ab99f42cc25f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11743 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa24dc23c274dd9924aa1bcf2de1e5a"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer = tokenizer,\n    model = model,\n    return_tensors = 'pt'\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:35:28.660131Z","iopub.execute_input":"2024-10-05T00:35:28.660464Z","iopub.status.idle":"2024-10-05T00:35:28.664838Z","shell.execute_reply.started":"2024-10-05T00:35:28.660410Z","shell.execute_reply":"2024-10-05T00:35:28.663817Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\n#model.gradient_checkpointing_enable()\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='./results',\n    eval_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps = 8,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=3,\n    fp16=True,\n    predict_with_generate=False,\n    logging_dir='./logs',\n    logging_steps=10,\n    generation_max_length = 128,\n    push_to_hub = False,\n    dataloader_num_workers=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:35:28.666051Z","iopub.execute_input":"2024-10-05T00:35:28.666339Z","iopub.status.idle":"2024-10-05T00:35:28.798501Z","shell.execute_reply.started":"2024-10-05T00:35:28.666308Z","shell.execute_reply":"2024-10-05T00:35:28.797710Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds_train,\n    eval_dataset=tokenized_ds_test,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:35:28.799773Z","iopub.execute_input":"2024-10-05T00:35:28.800480Z","iopub.status.idle":"2024-10-05T00:35:30.612154Z","shell.execute_reply.started":"2024-10-05T00:35:28.800421Z","shell.execute_reply":"2024-10-05T00:35:30.611200Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nwandb.disabled = True\nos.environ['WANDB_DISABLED'] = 'true'","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:35:30.616236Z","iopub.execute_input":"2024-10-05T00:35:30.616557Z","iopub.status.idle":"2024-10-05T00:35:30.621129Z","shell.execute_reply.started":"2024-10-05T00:35:30.616524Z","shell.execute_reply":"2024-10-05T00:35:30.620184Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T00:35:30.622706Z","iopub.execute_input":"2024-10-05T00:35:30.622994Z","iopub.status.idle":"2024-10-05T09:48:45.824204Z","shell.execute_reply.started":"2024-10-05T00:35:30.622964Z","shell.execute_reply":"2024-10-05T09:48:45.823298Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2568' max='2568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2568/2568 9:12:59, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.022100</td>\n      <td>0.016034</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.012500</td>\n      <td>0.013960</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.010800</td>\n      <td>0.013275</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2568, training_loss=0.10857972617310434, metrics={'train_runtime': 33193.6911, 'train_samples_per_second': 2.476, 'train_steps_per_second': 0.077, 'total_flos': 1.410375629461586e+17, 'train_loss': 0.10857972617310434, 'epoch': 2.999124087591241})"},"metadata":{}}]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:48:45.825863Z","iopub.execute_input":"2024-10-05T09:48:45.826353Z","iopub.status.idle":"2024-10-05T09:49:00.388529Z","shell.execute_reply.started":"2024-10-05T09:48:45.826307Z","shell.execute_reply":"2024-10-05T09:49:00.387367Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=fcc11b60eb0817a1bffecc1fdc5fd48c2519540e946fcd4445311fd2a3749292\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:49:00.390108Z","iopub.execute_input":"2024-10-05T09:49:00.390486Z","iopub.status.idle":"2024-10-05T09:49:12.483140Z","shell.execute_reply.started":"2024-10-05T09:49:00.390423Z","shell.execute_reply":"2024-10-05T09:49:12.481974Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom nltk.tokenize import RegexpTokenizer\n\nrouge_metric = evaluate.load(\"rouge\")\n\n# define function for custom tokenization\ndef tokenize_sentence(arg):\n    encoded_arg = tokenizer(arg)\n    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n\n# define function to get ROUGE scores with custom tokenization\ndef metrics_func(eval_arg):\n    preds, labels = eval_arg\n\n    # Replace -100\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    # Convert id tokens to text\n\n    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # Insert a line break (\\n) in each sentence for ROUGE scoring\n\n    # (Note : Please change this code, when you perform on other languages except for Bengali)\n    text_preds = [(p if p.endswith((\"!\", \"!\", \"?\", \"?\", \"।\")) else p + \"।\") for p in text_preds]\n    text_labels = [(l if l.endswith((\"!\", \"!\", \"?\", \"?\", \"।\")) else l + \"।\") for l in text_labels]\n    sent_tokenizer_bn = RegexpTokenizer(u'[^!!??।]*[!!??।]')\n    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(p))) for p in text_preds]\n    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(l))) for l in text_labels]\n\n    # compute ROUGE score with custom tokenization\n    return rouge_metric.compute(\n    predictions=text_preds,\n    references=text_labels,\n    tokenizer=tokenize_sentence\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:49:12.484803Z","iopub.execute_input":"2024-10-05T09:49:12.485123Z","iopub.status.idle":"2024-10-05T09:49:14.315075Z","shell.execute_reply.started":"2024-10-05T09:49:12.485090Z","shell.execute_reply":"2024-10-05T09:49:14.314128Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec956774643448f8da680a104109f9b"}},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:49:14.316295Z","iopub.execute_input":"2024-10-05T09:49:14.316623Z","iopub.status.idle":"2024-10-05T09:49:14.322294Z","shell.execute_reply.started":"2024-10-05T09:49:14.316591Z","shell.execute_reply":"2024-10-05T09:49:14.321061Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nsample_dataloader = DataLoader(\n    tokenized_ds_test.with_format(\"torch\"),\n    collate_fn=data_collator,\n    batch_size=1\n)\n\nfor batch in sample_dataloader:\n    with torch.no_grad():\n        preds = model.generate(\n            batch[\"input_ids\"].to(device),\n            num_beams=2,\n            num_return_sequences=1,\n            no_repeat_ngram_size=1,\n            remove_invalid_values=True,\n            max_length=128\n        )\n    labels = batch[\"labels\"]\n    break\n\nmetrics_func([preds, labels])","metadata":{"execution":{"iopub.status.busy":"2024-10-05T09:49:14.323625Z","iopub.execute_input":"2024-10-05T09:49:14.323926Z","iopub.status.idle":"2024-10-05T09:49:17.175598Z","shell.execute_reply.started":"2024-10-05T09:49:14.323894Z","shell.execute_reply":"2024-10-05T09:49:17.174675Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.4855491329479769,\n 'rouge2': 0.44444444444444453,\n 'rougeL': 0.4739884393063584,\n 'rougeLsum': 0.47457627118644075}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}